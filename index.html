<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Tone Adjustment, Image Enhancement, CLIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/hmin970922/">Hyeongmin Lee</a><sup>1,*</sup>,
            </span>
            <span class="author-block">
              <a href="https://kkang831.github.io/">Kyoungkook Kang</a><sup>2,*</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/jungseulok">Jungseul Ok</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.scho.pe.kr/">Sunghyun Cho</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">POSTECH <sup>1</sup>GSAI & <sup>2</sup>CSE</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">CVPR 2024</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Supple Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supple</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hmin970922/CLIPtone"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>
 -->
  
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <img src="./static/images/Teaser.png"
           class="interpolation-image"
           alt="Teaser."/>
      <!-- <h2 class="subtitle has-text-centered"> -->
      <h2 class="subtitle is-full-width">
        We present CLIPtone, a text-based image tone adjustment framework trained in an unsupervised manner.
        With its superior understanding of natural languages, CLIPtone is capable of performing successful adjustments across a range of text descriptions, including those previously deemed impossible.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent image tone adjustment (or enhancement) approaches have predominantly adopted supervised learning for learning human-centric perceptual assessment. However, these approaches are constrained by intrinsic challenges of supervised learning.
            Primarily, the requirement for expertly-curated or retouched images escalates the data acquisition expenses. Moreover, their coverage of target style is confined to stylistic variants inferred from the training data. 
          </p>
          <p>
            To surmount the above challenges, we propose an unsupervised learning-based approach for text-based image tone adjustment method, CLIPtone, that extends an existing image enhancement method to accommodate natural language descriptions. 
            Specifically, we design a hyper-network to adaptively modulate the pretrained parameters of the backbone model based on text description.
            To assess whether the adjusted image aligns with the text description without ground truth image, we utilize CLIP, which is trained on a vast set of language-image pairs and thus encompasses knowledge of human perception.
            The major advantages of our approach are three fold: (i) minimal data collection expenses, (ii) support for a range of adjustments, and (iii) the ability to handle novel text descriptions unseen in training.
            Our approach's efficacy is demonstrated through comprehensive experiments, including a user study.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
  
    <!--/ Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <!--/ Abstract. -->

    <!-- Method. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified"> -->    
      <div class="columns is-centered has-text-centered">      
        <div class="column is-full-width">
          <h2 class="title is-3">Method</h2>
          <img src="./static/images/Method.png"
                 class="interpolation-image"
                 alt="Network architecture."
                 width="100%"/>
          <div class="content has-text-justified">
          <p>
            CLIPtone consists of a text adaptor and a tone adjustment network.
            The text adaptor calculates a directional vector within CLIP embedding space from input text descriptions and estimates modulation parameters for the AdaInt module and the weight predictor of the tone adjustment network.
            The modulated tone adjustment network, pre-trained on an image enhancement dataset, adaptively constructs an image-text adaptive 3D LUT through a combination of basis 3D LUTs and non-uniform sampling, ultimately adjusting the color values of the input image.
          </p>
          </div>
        </div>
      </div>
    <!--/ Method. -->
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">      
      <div class="column is-full-width">
        <h2 class="title is-3">Comparisons</h2>
          <div class="content has-text-justified">
          <p>
            We built a portable hyperspectral polarimetric imaging system, consisting of a monochromatic camera, LCTF that controls the transmitted wavelength, 
            and a quarter wave plate mounted on a motorized rotation stage.
          </p>
          
          <img src="./static/images/imaging_system.PNG"
                 class="interpolation-image"
                 alt="Network architecture."
                 width="100%"/>


          </div>
      </div
    </div>
  </div>
</section>
 -->

<section class="hero is-light is-small">
  <div class="hero-body" style="display: flex; justify-content: center; align-items: center;">
    <div class="container" style="width: 50%;">
      <h2 class="title is-3" style="text-align: center;">Results</h2>
      <div id="results-carousel" class="carousel results-carousel" data-slides-to-show="1">
        <div class="item item-results1">
          <img src="./static/images/Results_1.png" id="Results_1" height="100%">
        </div>
        <div class="item item-results2">
          <img src="./static/images/Results_2.png" id="Results_2" height="100%">
        </div>
        <div class="item item-results3">
          <img src="./static/images/Results_3.png" id="Results_3" height="100%">
        </div>
        <div class="item item-results4">
          <img src="./static/images/Results_4.png" id="Results_4" height="100%">
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
